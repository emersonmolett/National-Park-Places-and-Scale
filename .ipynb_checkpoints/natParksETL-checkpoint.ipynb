{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies and setup\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python SQL toolkit and Object Relational Mapper\n",
    "import sqlalchemy\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine, inspect, func, desc, Column, Integer, String, Float\n",
    "from config import (user, password, host, port, database)\n",
    "# from TAconfig import (user, password, host, port, database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### A. &nbsp;National Parks (from JSON)\n",
    "- National Parks with correct lat/lng coordinates.\n",
    "- Incomplete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source URL and variable\n",
    "aURL = \"https://github.com/learn-chef/national-parks-java/blob/master/national-parks.json\"\n",
    "aFilePath = \"data/natParks.json\"\n",
    "print(f\"{aFilePath} is {round(os.path.getsize(aFilePath)/1024/1024, 2)} megabytes (MB).\\nMore info here:\\n{aURL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file into DataFrame\n",
    "a_df = pd.read_json(aFilePath)\n",
    "a_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. &nbsp;National Parks (from CSV)\n",
    "- All National Parks.\n",
    "- Complete dataset of National Park names, but missing 102 lat/lng coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source URL and variable\n",
    "bURL = \"https://public-nps.opendata.arcgis.com/datasets/nps-boundary-1/data?geometry=79.963%2C-20.479%2C-104.959%2C70.899\"\n",
    "bFilePath = \"data/natParks.csv\"\n",
    "print(f\"{bFilePath} is {round(os.path.getsize(bFilePath)/1024/1024, 2)} megabytes (MB).\\nMore info here:\\n{bURL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame\n",
    "bRaw_df = pd.read_csv(bFilePath)\n",
    "bRaw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. &nbsp;National Parks with Size/Area (from CSV)\n",
    "- All National Parks; complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source URL and variable\n",
    "cURL = \"https://public-nps.opendata.arcgis.com/datasets/nps-boundary-1/data?geometry=79.963%2C-20.479%2C-104.959%2C70.899\"\n",
    "cFilePath = \"data/natParksArea.csv\"\n",
    "print(f\"{cFilePath} is {round(os.path.getsize(cFilePath)/1024/1024, 2)} megabytes (MB).\\nMore info here:\\n{cURL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame\n",
    "cRaw_df = pd.read_csv(cFilePath)\n",
    "cRaw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points of Interest\n",
    "- All Points of Interest within each National Park with lat/lng coordinates.\n",
    "- Complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source URL and variable\n",
    "rawPointsURL = \"https://public-nps.opendata.arcgis.com/datasets/nps-points-of-interest-pois-geographic-coordinate-system-1?geometry=79.973%2C-21.130%2C-104.949%2C70.669\"\n",
    "rawPointsFilePath = \"data/pointsInterest.csv\"\n",
    "print(f\"{rawPointsFilePath} is {round(os.path.getsize(rawPointsFilePath)/1024/1024, 2)} megabytes (MB).\\nMore info here:\\n{rawPointsURL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame\n",
    "rawPoints_df = pd.read_csv(rawPointsFilePath, low_memory = False)\n",
    "rawPoints_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attendance/Visitors (Recreation Visits)\n",
    "- Dataset does not include attendance numbers for trails and other unstaffed National Park locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source URL and variable\n",
    "attendanceURL = \"https://irma.nps.gov/STATS/SSRSReports/National%20Reports/Annual%20Visitation%20By%20Park%20(1979%20-%20Last%20Calendar%20Year)\"\n",
    "attendanceFilePath = \"data/finalAttendance.csv\"\n",
    "print(f\"{attendanceFilePath} is {round(os.path.getsize(attendanceFilePath)/1024/1024, 2)} megabytes (MB).\\nMore info here:\\n{attendanceURL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame\n",
    "attendance_df = pd.read_csv(attendanceFilePath)\n",
    "attendance_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. &nbsp;National Parks (from JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Location Number (park code) column to all caps\n",
    "a_df[\"Location Number\"] = a_df[\"Location Number\"].str.upper()\n",
    "a_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "a_df.drop(columns = [\"Address\", \"City\", \"State\", \"Zip Code\", \"Phone Number\", \"Fax Number\", \"Location\"], inplace = True)\n",
    "a_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "a_df = a_df.rename(columns = {\"Location Number\":\"Code\", \"Location Name\":\"Name\"})\n",
    "a_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. &nbsp;National Parks (from CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame with only desired columns\n",
    "b_df = bRaw_df[[\"UNIT_CODE\", \"UNIT_NAME\"]].copy()\n",
    "b_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "b_df = b_df.rename(columns = {\"UNIT_CODE\":\"Code\", \"UNIT_NAME\":\"Name\"})\n",
    "b_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame by National Park name\n",
    "b_df.sort_values(by = [\"Name\"], ascending = True, inplace = True)\n",
    "b_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index in place\n",
    "b_df.reset_index(inplace = True, drop = True)\n",
    "b_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Merge A and B DataFrames to create one list of National Parks DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames\n",
    "natParksRaw_df = pd.merge(b_df, a_df, how = \"left\", left_on = \"Code\", right_on = \"Code\")\n",
    "natParksRaw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "natParksRaw_df.drop(columns = [\"Name_y\"], inplace = True)\n",
    "natParksRaw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "natParksRaw_df = natParksRaw_df.rename(columns = {\"Name_x\":\"Name\"})\n",
    "natParksRaw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame size\n",
    "natParksRaw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of unique values in Code column\n",
    "a = len(pd.unique(natParksRaw_df[\"Code\"]))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows with duplicate value in Code column\n",
    "duplicates_df = natParksRaw_df[natParksRaw_df.duplicated(\"Code\")]\n",
    "duplicates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with duplicates in Code column (acreage is totaled on first row of duplicates on natParksArea CSV)\n",
    "natParksRaw_df = natParksRaw_df.drop_duplicates(subset = \"Code\", keep = \"first\")\n",
    "natParksRaw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from rows with missing lat/lng values\n",
    "null_df = natParksRaw_df[natParksRaw_df.isnull().any(axis = 1)]\n",
    "null_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of unique values in Code column\n",
    "b = len(pd.unique(null_df[\"Code\"]))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export DataFrame to CSV for manual lat/lng entry\n",
    "# null_df.to_csv(\"data/missingCoordinates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame\n",
    "missingCoordinates_df = pd.read_csv(\"data/missingCoordinates.csv\", encoding = \"iso-8859-1\")\n",
    "missingCoordinates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted space from strings in Longitude column\n",
    "missingCoordinates_df[\"Longitude\"] = missingCoordinates_df[\"Longitude\"].str.replace(\" \", \"\")\n",
    "missingCoordinates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary column\n",
    "missingCoordinates_df.drop(columns = [\"Unnamed: 0\"], inplace = True)\n",
    "missingCoordinates_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Longitude column to float\n",
    "missingCoordinates_df[\"Longitude\"] = missingCoordinates_df[\"Longitude\"].astype(float)\n",
    "missingCoordinates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "missingCoordinates_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Merge National Parks and Missing Coordinates DataFrames*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames\n",
    "np_df = natParksRaw_df.merge(missingCoordinates_df, how = \"left\", left_on = \"Code\", right_on = \"Code\")\n",
    "np_df[\"Latitude_x\"].fillna(np_df[\"Latitude_y\"], inplace = True)\n",
    "np_df[\"Longitude_x\"].fillna(np_df[\"Longitude_y\"], inplace = True)\n",
    "np_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of unique values in Code column\n",
    "c = len(pd.unique(np_df[\"Code\"]))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "np_df.drop(columns = [\"Name_y\", \"Latitude_y\", \"Longitude_y\"], inplace = True)\n",
    "np_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "np_df = np_df.rename(columns = {\"Name_x\":\"Name\", \"Latitude_x\":\"Latitude\", \"Longitude_x\":\"Longitude\"})\n",
    "np_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame size\n",
    "np_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. &nbsp;National Parks with Size in Acres (from CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame with only desired columns\n",
    "c_df = cRaw_df[[\"UNIT_CODE\", \"UNIT_NAME\", \"Shape__Acreage\"]].copy()\n",
    "c_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "c_df = c_df.rename(columns = {\"UNIT_CODE\":\"Code\", \"UNIT_NAME\":\"Name\", \"Shape__Acreage\":\"Area\"})\n",
    "c_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame by National Park name\n",
    "c_df.sort_values(by = [\"Name\"], ascending = True, inplace = True)\n",
    "c_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index in place\n",
    "c_df.reset_index(inplace = True, drop = True)\n",
    "c_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of unique values in Code column\n",
    "d = len(pd.unique(c_df[\"Code\"]))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame size\n",
    "c_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Merge National Parks and National Parks Area DataFrames*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames\n",
    "natParksArea_df = np_df.merge(c_df, how = \"left\", left_on = \"Code\", right_on = \"Code\")\n",
    "natParksArea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of unique values in Code column\n",
    "e = len(pd.unique(natParksArea_df[\"Code\"]))\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "natParksArea_df.drop(columns = [\"Name_y\"], inplace = True)\n",
    "natParksArea_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "natParksArea_df = natParksArea_df.rename(columns = {\"Name_x\":\"Name\"})\n",
    "natParksArea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index in place\n",
    "natParksArea_df.reset_index(inplace = True, drop = True)\n",
    "natParksArea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame size\n",
    "natParksArea_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame to look for rows with NaN values\n",
    "nullArea_df = natParksArea_df[natParksArea_df.isnull().any(axis = 1)]\n",
    "nullArea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append acreage area to ALKA\n",
    "natParksArea_df[\"Area\"][5] = 424.242424"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. &nbsp;Points of Interest (from CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame with only desired columns\n",
    "allPoints_df = rawPoints_df[[\"MAPLABEL\", \"POITYPE\", \"Y\", \"X\"]].copy()\n",
    "allPoints_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "allPoints_df = allPoints_df.rename(columns = {\"MAPLABEL\":\"Name\", \"POITYPE\":\"Type\", \"Y\":\"Latitude\", \"X\":\"Longitude\"})\n",
    "allPoints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform blank string values to NaN values\n",
    "allPoints_df[\"Name\"].replace(\" \", np.nan, inplace = True)\n",
    "allPoints_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows containing NaN values\n",
    "allPoints_df.dropna(subset = [\"Name\"], inplace = True)\n",
    "allPoints_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index in place\n",
    "allPoints_df.reset_index(inplace = True, drop = True)\n",
    "allPoints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List all points of interest types\n",
    "# allPoints_df[\"Type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "allPoints_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parking Lots DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare boolean variable to filter by parking lots\n",
    "parkingLots = allPoints_df[\"Type\"] == \"Parking Lot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame filtered on parking lots\n",
    "parking_df = allPoints_df[parkingLots]\n",
    "parking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for SQL compatibility\n",
    "parking_df = parking_df.rename(columns = {\"Name\":\"name\", \"Type\":\"type\", \"Latitude\":\"latitude\", \"Longitude\":\"longitude\"})\n",
    "parking_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index in place\n",
    "parking_df.reset_index(inplace = True, drop = True)\n",
    "parking_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtered Points of Interest for Map Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame filtered on desired type values\n",
    "filteredPoints_df = allPoints_df[(allPoints_df.Type == \"ATM\") |\n",
    "                                 (allPoints_df.Type == \"Entrance / Exit\") |\n",
    "                                 (allPoints_df.Type == \"Fire Station\") |\n",
    "                                 (allPoints_df.Type == \"First Aid Station\") |\n",
    "                                 (allPoints_df.Type == \"Food Concession\") |\n",
    "                                 (allPoints_df.Type == \"Food Service\") |\n",
    "                                 (allPoints_df.Type == \"Gift Shop\") |\n",
    "                                 (allPoints_df.Type == \"InfoKiosk\") |\n",
    "                                 (allPoints_df.Type == \"Information\") |\n",
    "                                 (allPoints_df.Type == \"Information Board\") |\n",
    "                                 (allPoints_df.Type == \"Information Map\") |\n",
    "                                 (allPoints_df.Type == \"Police\") |\n",
    "                                 (allPoints_df.Type == \"Ranger Station\") |\n",
    "                                 (allPoints_df.Type == \"Shelter\") |\n",
    "                                 (allPoints_df.Type == \"Store\") |\n",
    "                                 (allPoints_df.Type == \"Visitor Center\") |\n",
    "                                 (allPoints_df.Type == \"Weather Shelter\")]\n",
    "filteredPoints_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for SQL compatibility\n",
    "filteredPoints_df = filteredPoints_df.rename(columns = {\"Name\":\"name\", \"Type\":\"type\", \"Latitude\":\"latitude\",\n",
    "                                                        \"Longitude\":\"longitude\"})\n",
    "filteredPoints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index in place\n",
    "filteredPoints_df.reset_index(inplace = True, drop = True)\n",
    "filteredPoints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "filteredPoints_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. &nbsp;Attendance/Visitors (Recreation Visits; from CSV)\n",
    "- By Park by Year\n",
    "- \"Presidents' Park\" did not have its own code; combined these attendance values with White House attendance numbers.\n",
    "- \"Klondike Gold Rush - Seattle\" technically falls under Klondike Gold Rush - Alaska and does not have its own code; eliminated this record altogether (average annual attendance is 66,907)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column\n",
    "attendance_df = attendance_df.rename(columns = {\"Park Name\":\"Name\"})\n",
    "attendance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of unique values in Code column\n",
    "f = len(pd.unique(attendance_df[\"Code\"]))\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "attendance_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform blank string values to NaN values\n",
    "attendance_df[\"2011\"].replace(\" \", np.nan, inplace = True)\n",
    "attendance_df[\"2012\"].replace(\" \", np.nan, inplace = True)\n",
    "attendance_df[\"2013\"].replace(\" \", np.nan, inplace = True)\n",
    "attendance_df[\"2014\"].replace(\" \", np.nan, inplace = True)\n",
    "attendance_df[\"2015\"].replace(\" \", np.nan, inplace = True)\n",
    "attendance_df[\"2016\"].replace(\" \", np.nan, inplace = True)\n",
    "attendance_df[\"2017\"].replace(\" \", np.nan, inplace = True)\n",
    "attendance_df[\"2018\"].replace(\" \", np.nan, inplace = True)\n",
    "attendance_df[\"2019\"].replace(\" \", np.nan, inplace = True)\n",
    "attendance_df[\"2020\"].replace(\" \", np.nan, inplace = True)\n",
    "attendance_df[\"Average\"].replace(\" \", np.nan, inplace = True)\n",
    "attendance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove commas from string values in yearly attendance columns\n",
    "attendance_df[\"2011\"] = attendance_df[\"2011\"].str.replace(\",\", \"\")\n",
    "attendance_df[\"2012\"] = attendance_df[\"2012\"].str.replace(\",\", \"\")\n",
    "attendance_df[\"2013\"] = attendance_df[\"2013\"].str.replace(\",\", \"\")\n",
    "attendance_df[\"2014\"] = attendance_df[\"2014\"].str.replace(\",\", \"\")\n",
    "attendance_df[\"2015\"] = attendance_df[\"2015\"].str.replace(\",\", \"\")\n",
    "attendance_df[\"2016\"] = attendance_df[\"2016\"].str.replace(\",\", \"\")\n",
    "attendance_df[\"2017\"] = attendance_df[\"2017\"].str.replace(\",\", \"\")\n",
    "attendance_df[\"2018\"] = attendance_df[\"2018\"].str.replace(\",\", \"\")\n",
    "attendance_df[\"2019\"] = attendance_df[\"2019\"].str.replace(\",\", \"\")\n",
    "attendance_df[\"2020\"] = attendance_df[\"2020\"].str.replace(\",\", \"\")\n",
    "attendance_df[\"Average\"] = attendance_df[\"Average\"].str.replace(\",\", \"\")\n",
    "attendance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform NaN values to \"0\"\n",
    "attendance_df[\"2011\"] = attendance_df[\"2011\"].fillna(0)\n",
    "attendance_df[\"2012\"] = attendance_df[\"2012\"].fillna(0)\n",
    "attendance_df[\"2013\"] = attendance_df[\"2013\"].fillna(0)\n",
    "attendance_df[\"2014\"] = attendance_df[\"2014\"].fillna(0)\n",
    "attendance_df[\"2015\"] = attendance_df[\"2015\"].fillna(0)\n",
    "attendance_df[\"2016\"] = attendance_df[\"2016\"].fillna(0)\n",
    "attendance_df[\"2017\"] = attendance_df[\"2017\"].fillna(0)\n",
    "attendance_df[\"2018\"] = attendance_df[\"2018\"].fillna(0)\n",
    "attendance_df[\"2019\"] = attendance_df[\"2019\"].fillna(0)\n",
    "attendance_df[\"2020\"] = attendance_df[\"2020\"].fillna(0)\n",
    "attendance_df[\"Average\"] = attendance_df[\"Average\"].fillna(0)\n",
    "attendance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform values to integers\n",
    "attendance_df[\"2011\"] = attendance_df[\"2011\"].astype(int)\n",
    "attendance_df[\"2012\"] = attendance_df[\"2012\"].astype(int)\n",
    "attendance_df[\"2013\"] = attendance_df[\"2013\"].astype(int)\n",
    "attendance_df[\"2014\"] = attendance_df[\"2014\"].astype(int)\n",
    "attendance_df[\"2015\"] = attendance_df[\"2015\"].astype(int)\n",
    "attendance_df[\"2016\"] = attendance_df[\"2016\"].astype(int)\n",
    "attendance_df[\"2017\"] = attendance_df[\"2017\"].astype(int)\n",
    "attendance_df[\"2018\"] = attendance_df[\"2018\"].astype(int)\n",
    "attendance_df[\"2019\"] = attendance_df[\"2019\"].astype(int)\n",
    "attendance_df[\"2020\"] = attendance_df[\"2020\"].astype(int)\n",
    "attendance_df[\"Average\"] = attendance_df[\"Average\"].astype(int)\n",
    "attendance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "attendance_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame size\n",
    "attendance_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Merge National Parks and Attendance DataFrames and create one comprehensive natParks DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames\n",
    "natParks_df = natParksArea_df.merge(attendance_df, how = \"left\", left_on = \"Code\", right_on = \"Code\")\n",
    "natParks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "natParks_df.drop(columns = [\"Name_y\"], inplace = True)\n",
    "natParks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for compatibility with SQL\n",
    "natParks_df = natParks_df.rename(columns = {\"Code\":\"code\", \"Name_x\":\"name\", \"Latitude\":\"latitude\", \"Longitude\":\"longitude\",\n",
    "                                            \"Area\":\"acres\", \"2011\":\"att_2011\", \"2012\":\"att_2012\", \"2013\":\"att_2013\",\n",
    "                                            \"2014\":\"att_2014\", \"2015\":\"att_2015\", \"2016\":\"att_2016\", \"2017\":\"att_2017\",\n",
    "                                            \"2018\":\"att_2018\", \"2019\":\"att_2019\", \"2020\":\"att_2020\", \"Average\":\"att_average\"})\n",
    "natParks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "natParks_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform NaN values to \"0\"\n",
    "natParks_df[\"att_2011\"] = natParks_df[\"att_2011\"].fillna(0)\n",
    "natParks_df[\"att_2012\"] = natParks_df[\"att_2012\"].fillna(0)\n",
    "natParks_df[\"att_2013\"] = natParks_df[\"att_2013\"].fillna(0)\n",
    "natParks_df[\"att_2014\"] = natParks_df[\"att_2014\"].fillna(0)\n",
    "natParks_df[\"att_2015\"] = natParks_df[\"att_2015\"].fillna(0)\n",
    "natParks_df[\"att_2016\"] = natParks_df[\"att_2016\"].fillna(0)\n",
    "natParks_df[\"att_2017\"] = natParks_df[\"att_2017\"].fillna(0)\n",
    "natParks_df[\"att_2018\"] = natParks_df[\"att_2018\"].fillna(0)\n",
    "natParks_df[\"att_2019\"] = natParks_df[\"att_2019\"].fillna(0)\n",
    "natParks_df[\"att_2020\"] = natParks_df[\"att_2020\"].fillna(0)\n",
    "natParks_df[\"att_average\"] = natParks_df[\"att_average\"].fillna(0)\n",
    "natParks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform values to integers\n",
    "natParks_df[\"att_2011\"] = natParks_df[\"att_2011\"].astype(int)\n",
    "natParks_df[\"att_2012\"] = natParks_df[\"att_2012\"].astype(int)\n",
    "natParks_df[\"att_2013\"] = natParks_df[\"att_2013\"].astype(int)\n",
    "natParks_df[\"att_2014\"] = natParks_df[\"att_2014\"].astype(int)\n",
    "natParks_df[\"att_2015\"] = natParks_df[\"att_2015\"].astype(int)\n",
    "natParks_df[\"att_2016\"] = natParks_df[\"att_2016\"].astype(int)\n",
    "natParks_df[\"att_2017\"] = natParks_df[\"att_2017\"].astype(int)\n",
    "natParks_df[\"att_2018\"] = natParks_df[\"att_2018\"].astype(int)\n",
    "natParks_df[\"att_2019\"] = natParks_df[\"att_2019\"].astype(int)\n",
    "natParks_df[\"att_2020\"] = natParks_df[\"att_2020\"].astype(int)\n",
    "natParks_df[\"att_average\"] = natParks_df[\"att_average\"].astype(int)\n",
    "natParks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "natParks_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrames to CSV files\n",
    "natParks_df.to_csv(\"resources/natParksFinal.csv\")\n",
    "filteredPoints_df.to_csv(\"resources/pointsFinal.csv\")\n",
    "parking_df.to_csv(\"resources/parkingFinal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export DataFrames to JSON files\n",
    "# natParks_df.to_json(\"resources/natParksFinal.json\")\n",
    "# filteredPoints_df.to_json(\"resources/pointsFinal.json\")\n",
    "# parking_df.to_json(\"resources/parkingFinal.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### postgreSQL Method (functioning correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an engine that can talk to the database\n",
    "engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{database}')\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for table\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to load DataFrames into natParks table\n",
    "natParks_df.to_sql(name = \"natparks\", con = engine, if_exists = \"append\", index = False)\n",
    "filteredPoints_df.to_sql(name = \"pointsinterest\", con = engine, if_exists = \"append\", index = False)\n",
    "parking_df.to_sql(name = \"parkinglots\", con = engine, if_exists = \"append\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below to be used for creating SQLite Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method 1. &nbsp;Insert Data with Classes and Constructors (10.2 Activities 1, 3, and 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sets an object to utilize the default declarative base in SQL Alchemy\n",
    "# Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create classes to define our tables\n",
    "# class Parks(Base):\n",
    "#     __tablename__ = \"natparks\"\n",
    "#     id = Column(Integer, primary_key = True)\n",
    "#     code = Column(String(255))\n",
    "#     name = Column(String(255))\n",
    "#     latitude = Column(Float)\n",
    "#     longitude = Column(Float)\n",
    "#     acres = Column(Float)\n",
    "#     att_2011 = Column(Integer)\n",
    "#     att_2012 = Column(Integer)\n",
    "#     att_2013 = Column(Integer)\n",
    "#     att_2014 = Column(Integer)\n",
    "#     att_2015 = Column(Integer)\n",
    "#     att_2016 = Column(Integer)\n",
    "#     att_2017 = Column(Integer)\n",
    "#     att_2018 = Column(Integer)\n",
    "#     att_2019 = Column(Integer)\n",
    "#     att_2020 = Column(Integer)\n",
    "#     att_average = Column(Integer)\n",
    "\n",
    "# class Points(Base):\n",
    "#     __tablename__ = \"pointsinterest\"\n",
    "#     id = Column(Integer, primary_key = True)\n",
    "#     name = Column(String(255))\n",
    "#     type = Column(String(255))\n",
    "#     latitude = Column(Float)\n",
    "#     longitude = Column(Float)\n",
    "    \n",
    "# class Parking(Base):\n",
    "#     __tablename__ = \"parkinglots\"\n",
    "#     id = Column(Integer, primary_key = True)\n",
    "#     name = Column(String(255))\n",
    "#     type = Column(String(255))\n",
    "#     latitude = Column(Float)\n",
    "#     longitude = Column(Float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NEED TO FIRST CREATE NATPARKS.SQLITE REFERENCED IN CELL BELOW???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create engine to natparks.sqlite\n",
    "# engine = create_engine(\"sqlite:///resources/natparks.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create our tables in the database\n",
    "# Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create our session (link) from Python to the DB\n",
    "# session = Session(bind = engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create instances of each class\n",
    "# parks = Parks(\"WHAT GOES IN HERE?\")  \"CONVERT DATAFRAMES TO DICTIONARY TO PASS IN HERE?\"\n",
    "# points = Points(\"WHAT GOES IN HERE?\")\n",
    "# lots = Parking(\"WHAT GOES IN HERE?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the SQL Alchemy methods to run simple \"INSERT\" statements using the classes and objects\n",
    "# session.add(Parks)\n",
    "# session.add(Points)\n",
    "# session.add(Parking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Commit transaction of changes to database\n",
    "# session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method 2. &nbsp;Insert Data with Pandas and SQLAlchemy ORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Insert Data with Pandas and SQLAlchemy ORM\n",
    "# Session = sessionmaker(bind = dest_db_con)\n",
    "# session = Session()\n",
    "# session.bulk_insert_mappings(natParks_df, df.to_dict(orient = \"records\"))\n",
    "# session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
